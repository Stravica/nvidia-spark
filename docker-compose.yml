name: "gpu-inference"

services:
  vllm-qwen3-32b-fp8:
    image: nvcr.io/nvidia/vllm:25.09-py3
    container_name: vllm-qwen3-32b-fp8
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      HF_TOKEN: "${HF_TOKEN}"
      HUGGING_FACE_HUB_TOKEN: "${HF_TOKEN}"
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    command:
      - vllm
      - serve
      - Qwen/Qwen3-32B-FP8
      - --download-dir
      - /root/.cache/huggingface
      - --max-model-len
      - "32000"
      - --gpu-memory-utilization
      - "0.90"
      - --max-num-batched-tokens
      - "16384"
      - --max-num-seqs
      - "64"
      - --enable-prefix-caching
      - --trust-remote-code
    volumes:
      - /opt/hf:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  vllm-qwen3-30b-a3b-fp8:
    image: nvcr.io/nvidia/vllm:25.09-py3
    container_name: vllm-qwen3-30b-a3b-fp8
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      HF_TOKEN: "${HF_TOKEN}"
      HUGGING_FACE_HUB_TOKEN: "${HF_TOKEN}"
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    command:
      - vllm
      - serve
      - Qwen/Qwen3-30B-A3B-Instruct-2507-FP8
      - --download-dir
      - /root/.cache/huggingface
      - --max-model-len
      - "32768"
      - --gpu-memory-utilization
      - "0.85"
      - --max-num-batched-tokens
      - "16384"
      - --max-num-seqs
      - "64"
      - --enable-prefix-caching
      - --trust-remote-code
    volumes:
      - /opt/hf:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  vllm-llama33-70b-fp8:
    image: nvcr.io/nvidia/vllm:25.09-py3
    container_name: vllm-llama33-70b-fp8
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      HF_TOKEN: "${HF_TOKEN}"
      HUGGING_FACE_HUB_TOKEN: "${HF_TOKEN}"
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    command:
      - vllm
      - serve
      - nvidia/Llama-3.3-70B-Instruct-FP8
      - --download-dir
      - /root/.cache/huggingface
      - --max-model-len
      - "65536"
      - --gpu-memory-utilization
      - "0.80"
      - --max-num-batched-tokens
      - "32768"
      - --max-num-seqs
      - "32"
      - --enable-prefix-caching
      - --trust-remote-code
    volumes:
      - /opt/hf:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  ollama-qwen3-32b-fp8:
    image: ollama/ollama:latest
    container_name: ollama-qwen3-32b-fp8
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      OLLAMA_KEEP_ALIVE: "-1"              # Never unload (match vLLM behavior)
      OLLAMA_HOST: "0.0.0.0:11434"
      OLLAMA_NUM_PARALLEL: "8"             # Concurrent requests
      OLLAMA_MAX_LOADED_MODELS: "1"        # Only Qwen3-32B
      OLLAMA_NUM_THREADS: "20"             # Match ARM cores
    volumes:
      - /opt/ollama:/root/.ollama
      - ./models/ollama/Modelfile-qwen3-32b-fp8:/root/Modelfile-qwen3-32b-fp8:ro
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        ollama serve &
        sleep 5
        ollama pull qwen3:32b-q8_0
        ollama create qwen3-32b-fp8 -f /root/Modelfile-qwen3-32b-fp8
        wait
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
